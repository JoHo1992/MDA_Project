{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM model (paragraphs as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm \n",
    "\n",
    "#Data structure manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#text cleaning \n",
    "import re\n",
    "import string\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.wrappers.dtmmodel import DtmModel\n",
    "\n",
    "# unlist nested lists\n",
    "from itertools import chain\n",
    "\n",
    "# count word frequencies\n",
    "from collections import defaultdict\n",
    "\n",
    "# vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# timer function\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to where the raw text files are stored within the session folders, i.e, converted sessions.\n",
    "origin_path = \"C:/DATA/convSes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for reading in the speeches\n",
    "def read_text(file_path, file):\n",
    "    \n",
    "    '''Reading the text files'''\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        doc=file.read()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec.model\n",
      "Session 74 - 2019 - Shortcut.lnk\n"
     ]
    }
   ],
   "source": [
    "#Bringing in all the speeches\n",
    "doc_set = []\n",
    "for i in range (0,50):\n",
    "    year = 1970 + i\n",
    "    session = \"session \" + str(25+i)+ \" - \"  + str(year)\n",
    "    data_path = f\"{origin_path}\\\\{session}\"\n",
    "    os.chdir(data_path)\n",
    "    for file in os.listdir():\n",
    "        if file[-4:]=='.txt':\n",
    "            file_path = f\"{data_path}\\\\{file}\"\n",
    "            doc_set.append({'Year': year, 'ISO_Code': file[:3] , 'text': read_text(file_path, file)})\n",
    "        else:\n",
    "            print(file)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8288/8288 [00:04<00:00, 1954.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO_Code</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>ALB</td>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>ARG</td>\n",
       "      <td>177.\\t : It is a fortunate coincidence that pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>AUS</td>\n",
       "      <td>100.\\t  It is a pleasure for me to extend to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>AUT</td>\n",
       "      <td>155.\\t  May I begin by expressing to Ambassado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>BEL</td>\n",
       "      <td>176. No doubt each of us, before coming up to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year ISO_Code                                               text\n",
       "0  1970      ALB  33: May I first convey to our President the co...\n",
       "1  1970      ARG  177.\\t : It is a fortunate coincidence that pr...\n",
       "2  1970      AUS  100.\\t  It is a pleasure for me to extend to y...\n",
       "3  1970      AUT  155.\\t  May I begin by expressing to Ambassado...\n",
       "4  1970      BEL  176. No doubt each of us, before coming up to ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['Year', 'ISO_Code', 'text']\n",
    "dataset = pd.concat([pd.DataFrame([i], columns=columns) for i in tqdm(doc_set)], ignore_index=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating subset including only the G20 states**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select states belonging to the G20 group (minus south afrika and EU representatives)\n",
    "g20 =  dataset.ISO_Code.isin(['CAN','FRA', 'DEU', 'USA', 'GBR', 'ITA', 'JPN','ARG', 'Aus', 'BRA', 'IND', 'IDN', \n",
    "                        'CAN', 'MEX', 'RUS', 'SAU', 'KOR', 'TUR', 'CHN'])\n",
    "G20 = dataset[g20]\n",
    "\n",
    "# reset ascending index for subset dataset\n",
    "G20.reset_index(inplace = True, drop = True)\n",
    "G20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a random sample of speeches\n",
    "G20.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating subsets for the 70s,80s,90s,2000s,2010s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G20_70s = G20[(G20['Year'] >= 1970) & (G20['Year'] < 1980)] \n",
    "G20_80s = G20[(G20['Year'] >= 1980) & (G20['Year'] < 1990)]\n",
    "G20_90s = G20[(G20['Year'] >= 1990) & (G20['Year'] < 2000)]\n",
    "G20_2000s = G20[(G20['Year'] >= 2000) & (G20['Year'] < 2010)]\n",
    "G20_2010s = G20[(G20['Year'] >= 2010) & (G20['Year'] < 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split speeches for every decade slice into paragraphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70s\n",
    "\n",
    "# Split speeches by delimiter \\n and...\n",
    "# ...stack a new row for each paragraph to dataframe... \n",
    "# ...with context info from corresponding speech.\n",
    "\n",
    "temporary_file = G20_70s['text'].str.split('\\.\\s?\\s?\\s?\\n', expand=True).stack().reset_index(level=0)\n",
    "temporary_file.rename(columns={'level_0': 'speech_index', 0: 'text'}, inplace=True)\n",
    "\n",
    "# merge context info from speeches with texts of paragraphs\n",
    "G20_70s = G20_70s.drop('text', 1) # drop original text column\n",
    "G20_70s = temporary_file.merge(G20_70s, right_index=True, left_on='speech_index', how='outer')\n",
    "\n",
    "# reorder columns in new datset\n",
    "G20_70s['paragraph_index'] = np.arange(len(G20_70s))\n",
    "cols = G20_70s.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "G20_70s = G20_70s[cols].set_index('paragraph_index')\n",
    "G20_70s = G20_70s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80s\n",
    "\n",
    "temporary_file = G20_80s['text'].str.split('\\.\\s?\\s?\\s?\\n', expand=True).stack().reset_index(level=0)\n",
    "temporary_file.rename(columns={'level_0': 'speech_index', 0: 'text'}, inplace=True)\n",
    "\n",
    "# merge context info from speeches with texts of paragraphs\n",
    "G20_80s = G20_80s.drop('text', 1) # drop original text column\n",
    "G20_80s = temporary_file.merge(G20_80s, right_index=True, left_on='speech_index', how='outer')\n",
    "\n",
    "# reorder columns in new datset\n",
    "G20_80s['paragraph_index'] = np.arange(len(G20_80s))\n",
    "cols = G20_80s.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "G20_80s = G20_80s[cols].set_index('paragraph_index')\n",
    "G20_80s = G20_80s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90s\n",
    "\n",
    "temporary_file = G20_90s['text'].str.split('\\.\\s?\\s?\\s?\\n', expand=True).stack().reset_index(level=0)\n",
    "temporary_file.rename(columns={'level_0': 'speech_index', 0: 'text'}, inplace=True)\n",
    "\n",
    "\n",
    "# merge context info from speeches with texts of paragraphs\n",
    "G20_90s = G20_90s.drop('text', 1) # drop original text column\n",
    "G20_90s = temporary_file.merge(G20_90s, right_index=True, left_on='speech_index', how='outer')\n",
    "\n",
    "# reorder columns in new datset\n",
    "G20_90s['paragraph_index'] = np.arange(len(G20_90s))\n",
    "cols = G20_90s.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "G20_90s = G20_90s[cols].set_index('paragraph_index')\n",
    "G20_90s = G20_90s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000s\n",
    "\n",
    "temporary_file = G20_2000s['text'].str.split('\\.\\s?\\s?\\s?\\n', expand=True).stack().reset_index(level=0)\n",
    "temporary_file.rename(columns={'level_0': 'speech_index', 0: 'text'}, inplace=True)\n",
    "\n",
    "\n",
    "# merge context info from speeches with texts of paragraphs\n",
    "G20_2000s = G20_2000s.drop('text', 1) # drop original text column\n",
    "G20_2000s = temporary_file.merge(G20_2000s, right_index=True, left_on='speech_index', how='outer')\n",
    "\n",
    "# reorder columns in new datset\n",
    "G20_2000s['paragraph_index'] = np.arange(len(G20_2000s))\n",
    "cols = G20_2000s.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "G20_2000s = G20_2000s[cols].set_index('paragraph_index')\n",
    "G20_2000s = G20_2000s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2010s\n",
    "\n",
    "temporary_file = G20_2010s['text'].str.split('\\.\\s?\\s?\\s?\\n', expand=True).stack().reset_index(level=0)\n",
    "temporary_file.rename(columns={'level_0': 'speech_index', 0: 'text'}, inplace=True)\n",
    "# temporary_file.reset_index().drop('index',1)\n",
    "\n",
    "# merge context info from speeches with texts of paragraphs\n",
    "G20_2010s = G20_2010s.drop('text', 1) # drop original text column\n",
    "G20_2010s = temporary_file.merge(G20_2010s, right_index=True, left_on='speech_index', how='outer')\n",
    "\n",
    "# reorder columns in new datset\n",
    "G20_2010s['paragraph_index'] = np.arange(len(G20_2010s))\n",
    "cols = G20_2010s.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "G20_2010s = G20_2010s[cols].set_index('paragraph_index')\n",
    "G20_2010s = G20_2010s[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining/loading in the needed functions --> can we do this in a cleaner way??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')#run in conda to download the library --> python -m download en_core_web_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_proc(text, stop_words=[]):\n",
    "    \n",
    "    '''Pre-processing the input in single list'''\n",
    "    \n",
    "    stops = stopwords.words(\"english\")\n",
    "    stops.extend(stop_words)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\t', ' ', text)\n",
    "    text = re.sub(r'\\-', ' ', text)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = re.sub(r'[0-9]+','', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    lower = text.lower()\n",
    "    doc = nlp(lower)\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma not in stops:\n",
    "                words.append(lemma)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =['general', 'assembly', 'conference', 'session', 'congratulations', \n",
    "             'congratulate', 'secretarygeneral','members', 'member', 'united', 'nations', \n",
    "             'nation', 'statement', 'honour','every', 'sir', 'majesty', 'president', \n",
    "             'minister', 'prime', 'ambassador', 'thank', 'thanks', 'world', 'international', \n",
    "             'states', 'we', 'us', 'they', 'system', 'organization','say', 'think', 'know', \n",
    "             'want', 'need', 'let', 'ask', 'go', 'look', 'stand', 'open', 'give', 'see', 'come', \n",
    "             'make', 'made', 'meet','act', 'use', 'take', 'bring', 'ensure', 'able', 'assume', \n",
    "             'continue', 'change', 'progress', 'process', 'year', 'years', 'time', 'today',  \n",
    "             'would', 'will', 'might', 'together', 'common', 'future', 'one', 'order', 'end', \n",
    "             'new', 'necessary', 'major', 'minor', 'many', 'people', 'peoples', 'appropriate', \n",
    "             'historic', 'adequate', 'best', 'better', 'confident', 'important', 'special',\n",
    "             'great', 'therefore', 'thus', 'hence', 'like', 'particularly', 'many', 'much', \n",
    "             'greater', 'especially', 'towards', 'always', 'whether', 'around',\n",
    "             'possible', 'clear', 'simply', 'must', 'also', 'however', 'mr',\n",
    "             'united', 'kingdom', 'great', 'britain', 'france', 'germany','italy', 'japan',\n",
    "             'canada', 'usa', 'argentina', 'australia', 'china', 'brazil', 'india', 'indonesia',  \n",
    "             'mexico', 'russia', 'saudi', 'arabia', 'south', 'korea', 'turkey','liechtenstein',\n",
    "             'I', ' ', '  ', 'still','could','may','thirdly','elsewhere','yet','ever','since','welcome','npt','among','non','secretary_kofi_annan',\n",
    "             'behalf','sixty', 'sixtieth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the bigram models from the bigram_trigram_model_notebooks\n",
    "from gensim.models.phrases import Phrases\n",
    "bigram_phraser = Phrases.load(\"C:/DATA/DTM/phrasers/bigram_speeches\")\n",
    "trigam_phraser = Phrases.load(\"C:/DATA/DTM/phrasers/trigram_speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess speeches and also detect bi and trigrams\n",
    "def pre_proc_comb(corpus,stop_words=[]):\n",
    "    \n",
    "    '''Looping the pre-processing over a list. Also checks if input is correct'''\n",
    "    \n",
    "    l = []\n",
    "    if isinstance(corpus, str):\n",
    "        l.append(init_proc(corpus,stop_words))\n",
    "    elif all(isinstance(s, str) for s in corpus):    \n",
    "        for item in tqdm(corpus):\n",
    "            l.append(init_proc(item,stop_words))\n",
    "    else:\n",
    "        print(\"Error: This function only accepts strings or a list of strings.\")\n",
    "    bigram_token = []\n",
    "    for j in l:\n",
    "        bigram_token.append(bigram_phraser[j])\n",
    "    trigram_token = []\n",
    "    for i in bigram_token:\n",
    "        trigram_token.append(trigam_phraser[i])\n",
    "    return trigram_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the 5 dataslices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70s\n",
    "text_corpus_70s = G20_70s['text'].values.tolist()\n",
    "processed_corpus_70s = pre_proc_comb(text_corpus_70s,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80s\n",
    "text_corpus_80s = G20_80s['text'].values.tolist()\n",
    "processed_corpus_80s = pre_proc_comb(text_corpus_80s,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90s\n",
    "text_corpus_90s = G20_90s['text'].values.tolist()\n",
    "processed_corpus_90s = pre_proc_comb(text_corpus_90s,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000s\n",
    "text_corpus_2000s = G20_2000s['text'].values.tolist()\n",
    "processed_corpus_2000s = pre_proc_comb(text_corpus_2000s,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2010s\n",
    "text_corpus_2010s = G20_2010s['text'].values.tolist()\n",
    "processed_corpus_2010s = pre_proc_comb(text_corpus_2010s,stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LDA models for every decade sice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The model files can be found on the github. If the user has the model file, the user can just run the notebook. If the user would like to run and train the model, he/she should remove the '#' in front of the model fitting and saving segments in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Placing the processed corpus in necessary format for LDA\n",
    "\n",
    "dictionary_70s = corpora.Dictionary(processed_corpus_70s)\n",
    "doc_term_matrix_70s = [dictionary_70s.doc2bow(speech) for speech in processed_corpus_70s]\n",
    "\n",
    "# Creating the LDA object \n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#building the model\n",
    "#lda_model_70s = LDA(corpus = doc_term_matrix_70s, id2word = dictionary_70s, num_topics = 10, random_state=100,\n",
    "#               chunksize= 1000, passes = 1000, iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model locally \n",
    "#temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/70s\")\n",
    "#lda_model_70s.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model if it is already built\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/70s\")\n",
    "lda_model_70s = LDA.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the word distribution over the different topics\n",
    "lda_model_70s.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis70s = gensimvis.prepare(lda_model_70s,doc_term_matrix_70s,dictionary_70s,mds= 'mmds', R=30)\n",
    "vis70s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Placing the processed corpus in necessary format for LDA\n",
    "\n",
    "dictionary_80s = corpora.Dictionary(processed_corpus_80s)\n",
    "doc_term_matrix_80s = [dictionary_80s.doc2bow(speech) for speech in processed_corpus_80s]\n",
    "\n",
    "# Creating the LDA object \n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#building the model\n",
    "#lda_model_80s = LDA(corpus = doc_term_matrix_80s, id2word = dictionary_80s, num_topics = 10, random_state=100,\n",
    "#               chunksize= 1000, passes = 1000, iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model locally \n",
    "#temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/80s\")\n",
    "#lda_model_80s.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model if it is already built\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/80s\")\n",
    "lda_model_80s = LDA.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the word distribution over the different topics\n",
    "lda_model_80s.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis80s = gensimvis.prepare(lda_model_80s,doc_term_matrix_80s,dictionary_80s,mds= 'mmds', R=30)\n",
    "vis80s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Placing the processed corpus in necessary format for LDA\n",
    "\n",
    "dictionary_90s = corpora.Dictionary(processed_corpus_90s)\n",
    "doc_term_matrix_90s = [dictionary_90s.doc2bow(speech) for speech in processed_corpus_90s]\n",
    "\n",
    "# Creating the LDA object \n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#building the model\n",
    "#lda_model_90s = LDA(corpus = doc_term_matrix_90s, id2word = dictionary_90s, num_topics = 10, random_state=100,\n",
    "#               chunksize= 1000, passes = 1000, iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model locally \n",
    "#temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/90s\")\n",
    "#lda_model_90s.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model if it is already built\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/90s\")\n",
    "lda_model_90s = LDA.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the word distribution over the different topics\n",
    "lda_model_90s.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis90s = gensimvis.prepare(lda_model_90s,doc_term_matrix_90s,dictionary_90s,mds= 'mmds', R=30)\n",
    "vis90s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Placing the processed corpus in necessary format for LDA\n",
    "\n",
    "dictionary_2000s = corpora.Dictionary(processed_corpus_2000s)\n",
    "doc_term_matrix_2000s = [dictionary_2000s.doc2bow(speech) for speech in processed_corpus_2000s]\n",
    "\n",
    "# Creating the LDA object \n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#building the model\n",
    "#lda_model_2000s = LDA(corpus = doc_term_matrix_2000s, id2word = dictionary_2000s, num_topics = 10, random_state=100,\n",
    "#               chunksize= 1000, passes = 1000, iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model locally \n",
    "#temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/2000s\")\n",
    "#lda_model_2000s.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model if it is already built\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/2000s\")\n",
    "lda_model_2000s = LDA.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the word distribution over the different topics\n",
    "lda_model_2000s.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis2000s = gensimvis.prepare(lda_model_2000s,doc_term_matrix_2000s,dictionary_2000s,mds= 'mmds', R=30)\n",
    "vis2000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Placing the processed corpus in necessary format for LDA\n",
    "\n",
    "dictionary_2010s = corpora.Dictionary(processed_corpus_2010s)\n",
    "doc_term_matrix_2010s = [dictionary_2010s.doc2bow(speech) for speech in processed_corpus_2010s]\n",
    "\n",
    "# Creating the LDA object \n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#building the model\n",
    "#lda_model_2010s = LDA(corpus = doc_term_matrix_2010s, id2word = dictionary_2010s, num_topics = 10, random_state=100,\n",
    "#               chunksize= 1000, passes = 1000, iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model locally \n",
    "#temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/2010s\")\n",
    "#lda_model_2010s.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model if it is already built\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "temp_file = datapath(\"C:/DATA/DTM/models/LDA_speeches/2010s\")\n",
    "lda_model_2010s = LDA.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the word distribution over the different topics\n",
    "lda_model_2010s.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis2010s = gensimvis.prepare(lda_model_2010s,doc_term_matrix_2010s,dictionary_2010s,mds= 'mmds', R=30)\n",
    "vis2010s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
